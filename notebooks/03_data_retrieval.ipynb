{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Data Retrieval\n",
    "\n",
    "This module shows how to access local data, and how to retrieve files through the assets API, including zarr storage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Data\n",
    "Once you have started up your Workspace, you might notice the _datasets_ folder. This contains datasets that are linked to the Workspace. The files in this folder are accessible in the same way as on your local machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas requests anndata zarr aiohttp fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import zarr\n",
    "import anndata as ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module we will reuse the same dataset from last module, as well as a new one. You can also inspect this dataset on the [Dataset Detail Page](https://portal.hubmapconsortium.org/browse/dataset/a1d17fdd270a69c813b872a927dfa5f3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuids = ['69c70762689b20308bb049ac49653342', 'a1d17fdd270a69c813b872a927dfa5f3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's access some local data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = ad.read_h5ad('./datasets/' + uuids[1] + '/secondary_analysis.h5ad')\n",
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This folder is read-only! So if you create outputs based on each dataset, will have to create a separate folder for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the assets API\n",
    "Perhaps you want to export your notebook, and so you want to include a way to load the data from the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://assets.hubmapconsortium.org/' + uuids[0] + '/' + 'sprm_outputs/reg001_expr.ome.tiff-SPRM_Image_Quality_Measures.json'\n",
    "\n",
    "res = requests.get(url)\n",
    "\n",
    "with open('./quality.json', mode='wb') as f:     \n",
    "    f.write(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now retrieved the first data product of our dataset, and written it to _quality.json_. We can open the file and see what it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.load(open('./quality.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can also download a lot more files. Below shows a function to download any file from a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_files_remote(uuid, file_name, outdir='.'): \n",
    "    '''\n",
    "    For a given UUID and file name, retrieve this file and save it locally.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    uuid : str\n",
    "        UUID of dataset\n",
    "    file_name : str\n",
    "        relative location of desired file. \n",
    "    outdir : str, optional\n",
    "        name of output folder. Default: '.'\n",
    "    '''\n",
    "    url = 'https://assets.hubmapconsortium.org/' + uuid + '/' + file_name\n",
    "\n",
    "    extension = str.split(file_name, sep='.')[-1]\n",
    "\n",
    "    # check if relative file_name has multiple subfolders\n",
    "    # if so, extract the folder structure without the filename as a string\n",
    "    folder_structure = str.split(file_name, sep='/')[0:-1]\n",
    "    folder_structure_addition = '/' + '/'.join(folder_structure) + '/' if len(folder_structure) > 0 else ''\n",
    "\n",
    "    if extension == 'h5ad':\n",
    "        warnings.warn('Large files such as .h5ad files may take long to retrieve.')\n",
    "    \n",
    "    res = requests.get(url)\n",
    "\n",
    "    if not os.path.exists(outdir + '/' + uuid + folder_structure_addition):\n",
    "        os.makedirs(outdir + '/' + uuid + folder_structure_addition, exist_ok = True) # unlike os.mkdir, os.makedirs creates directories recursively\n",
    "\n",
    "    with open(outdir + '/' + uuid + '/' + file_name, mode='wb') as f:     \n",
    "        f.write(res.content)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Module 2, we showed how to retrieve files for a dataset. Let's reuse that functionality here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_for_uuids(uuids, search_api='https://search.api.hubmapconsortium.org/v3/portal/search'):\n",
    "    '''\n",
    "    Create a dictionary of files per dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    uuid : array of str or str\n",
    "        UUID(s) of dataset(s)\n",
    "    search_api : str, optional\n",
    "        URL of search_api. Default: 'https://search.api.hubmapconsortium.org/v3/portal/search'\n",
    "    '''\n",
    "    hits = json.loads(\n",
    "        requests.post(\n",
    "            search_api,\n",
    "            json={\n",
    "                \"size\": 10000,\n",
    "                \"query\": {\"ids\": {\"values\": uuids}},\n",
    "                \"_source\": [\"files\"]\n",
    "            }, \n",
    "        ).text\n",
    "    )[\"hits\"][\"hits\"]\n",
    "\n",
    "    uuid_to_files = {}\n",
    "    for hit in hits:\n",
    "        file_paths = [file['rel_path'] for file in hit['_source']['files']]\n",
    "        uuid_to_files[hit['_id']] = file_paths\n",
    "\n",
    "    return uuid_to_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid_to_files = get_files_for_uuids(uuids)\n",
    "\n",
    "# Run to download all files from this dataset.\n",
    "# This takes a few minutes.\n",
    "# for file in uuid_to_files[uuids[0]]: \n",
    "#     retrieve_files_remote(uuids[0], file, outdir='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zarr\n",
    "H5ad files are very large and can take a long time to retrieve. [Zarr](https://zarr.readthedocs.io/) is a storage format for N-dimensional arrays, which significantly speeds up loading times. This is why many files in HuBMAP datasets are indexed as Zarr files. We can load these objects through the remote Zarr storage.\n",
    "\n",
    "In the files overview, you may have already seen some Zarr files, such as _anndata-zarr/reg001_expr-anndata.zarr/.zgroup_. These _.zgroup_ files do not contain any data, but are created when the Zarr groups are created, and indicate that these stores are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the zarr_url for this dataset and file\n",
    "zarr_url = f'https://assets.hubmapconsortium.org/{uuids[1]}/hubmap_ui/anndata-zarr/secondary_analysis.zarr'\n",
    "\n",
    "# get the X array\n",
    "X_arr = zarr.open(zarr_url + \"/X\")\n",
    "\n",
    "# load as pandas DataFrame\n",
    "X_df = pd.DataFrame(X_arr)\n",
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps we are very familiar with Anndata or the dataset's pipelines (as mentioned in Module 2) and we know exactly the Zarr stores that we want to retrieve. If that's not the case, usually, we would be able to retrieve the Zarr hierarchy through [_zarr.hierarchy_](https://zarr.readthedocs.io/en/stable/api/hierarchy.html). However, since this is a remote store, this is not possible. We can however use the existence of these _.zgroup_ files to figure out the structure of the Zarr files.\n",
    "\n",
    "We can modify our file retrieval to get all the Zarr paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zarr_paths(uuids, search_api = 'https://search.api.hubmapconsortium.org/v3/portal/search'):\n",
    "    '''\n",
    "    Get dictionary of zarr extensions for datasets.\n",
    "    For each dataset, it has a new dictionary, with the base zarr storages as keys, and\n",
    "    extensions as a list for it's value. \n",
    "    The base zarr storages can also be interpreted as the different anndata files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    uuids : list of str\n",
    "        list with dataset UUIDs\n",
    "    search_api : str, optional\n",
    "        URL of HuBMAP Search API. Default: 'https://search.api.hubmapconsortium.org/v3/portal/search'\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dictionary with for each UUID a new dictionary with base zarr stores and extensions\n",
    "    '''\n",
    "    hits = json.loads(\n",
    "            requests.post(\n",
    "                search_api,\n",
    "                json={\n",
    "                    'size': 10000,\n",
    "                    'query': {'ids': {'values': uuids}},\n",
    "                    '_source': ['files']\n",
    "                }, \n",
    "            ).text\n",
    "        )['hits']['hits']\n",
    "\n",
    "    uuid_to_files = {}\n",
    "    for hit in hits:\n",
    "        # get all the file_paths for a dataset\n",
    "        file_paths = [file['rel_path'] for file in hit['_source']['files']]\n",
    "\n",
    "        # filter file_paths for zarr\n",
    "        file_paths_zarr = [file_name for file_name in file_paths if 'zarr' in file_name]\n",
    "        \n",
    "        # get the roots of the zarr groups\n",
    "        root_files = [file_name.replace('.zarr/.zgroup', '') for file_name in file_paths_zarr if '.zarr/.zgroup' in file_name]\n",
    "\n",
    "        # create a dictionary from root to extension\n",
    "        root_files_to_files = {root_file : [file.replace(root_file + '.zarr/', '') for file in file_paths_zarr if root_file in file] for root_file in root_files}\n",
    "        \n",
    "        uuid_to_files[hit['_id']] = root_files_to_files\n",
    "    \n",
    "    return uuid_to_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_paths = get_zarr_paths(uuids)[uuids[1]]\n",
    "zarr_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it for yourself!\n",
    "We can use this file overview and our _zarr.open_ function to retrieve all Zarr stores. We can even automate this to retrieve the entire anndata object for datasets way faster than through the .h5ad file. Try it yourself! If you want a hint, you can look at the _load\\_zarr_ template in the Portal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
